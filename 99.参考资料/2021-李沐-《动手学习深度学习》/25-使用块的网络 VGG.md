## 25 使用块的网络 VGG

### 目录

- [25 使用块的网络 VGG](#25-使用块的网络-vgg)
  - [目录](#目录)
  - [1. VGG 块](#1-vgg-块)
  - [2. VGG 架构](#2-vgg-架构)
  - [3. 总结](#3-总结)
  - [4. QA](#4-qa)

Alexnet 最大的问题在于长得不规则，结构不甚清晰，也不便于调整。想要把网络做的更深更大需要更好的设计思想和标准框架。

### 1. VGG 块

直到现在更深更大的模型也是我们努力的方向，在当时 AlexNet 比 LeNet 更深更大得到了更好的精度，大家也希望把网络做的更深更大。选择之一是使用更多的全连接层，但全连接层的成本很高；第二个选择是使用更多的卷积层，但缺乏好的指导思想来说明在哪加，加多少。最终 VGG 采取了将卷积层组合成块，再把卷积块组合到一起的思路。

VGG 块可以看作是 AlexNet 思路的拓展，AlexNet 中将三个相同的卷积层放在一起再加上一个池化层，而 VGG 将其拓展成可以使用任意个 3x3，不改变输入大小的的卷积层，最后加上一个 2x2 的最大池化层。

<img src="..\imgs\25\25-01.PNG" alt="25-01" style="zoom:25%;" />

为什么选择 3x3 卷积呢？在计算量相同的情况下选用更大的卷积核涉及对网络会越浅，VGG 作者经过实验发现用 3x3 卷积的效果要比 5x5 好，也就是说神经网络库深且窄的效果会更好。

### 2. VGG 架构

多个 VGG 块后接全连接层，不同次数的重复块得到不同的架构，如 VGG-16, VGG-19 等，后面的数字取决于网络层数。

可以讲 VGG 看作是将 AlexNet 中连续卷积的部分取出加以推广和复制，并删去了 AlexNet 中不那么规整的前几层。

<img src="..\imgs\25\25-02.PNG" alt="25-02" style="zoom:25%;" />

VGG 较 AlexNet 相比性能有很大的提升，而代价是处理样本速度的降低和内存占用的增加。

### 3. 总结

- VGG 使用可重复使用的卷积块来构建深度卷积网络

- 不同卷积块个数和超参数可以得到不同复杂度的变种

这些思想影响了后面神经网络的设计，在之后的模型中被广泛使用。

### 4. QA

Q1: 视觉领域人工特征的研究还有无进展？

> 现在在计算机视觉做人工特征是一种“政治不正确”的事，可能会因被认为没有 novelty 而发不出 paper ;-)
>
> 老师认为人工特征提取确实应该被取代掉，随着技术进步可以把这部分工作交给机器，人去做更高级的事。

Q2: 需要学习特征值/特征向量/奇异值分解的知识吗？

> 这门课中不一定会讲，但很多深度学习模型用到矩阵分解的思想，但是用的不多，想学可以学。

Q3: Colab 限时 12 小时与验证码的解决方法

> 充钱

Q4: 训练 loss 一直下降，测试 loss 一只不降的原因

> 代码写错了/过拟合(训练集和测试集很不一样)

Q5: 为什么 VGG（1，1，224，224）输入高宽减半后通道数是 64？

> 第一个卷积层的输出通道选的是 64。(通道数变化是自定的，和高宽变化没有关系)
